{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7aa6aac-2d9a-4d6e-b828-6aea6664f053",
   "metadata": {},
   "source": [
    "## Foundational Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c5cd5-db6c-430e-b4ef-59a16bc840ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "def build_generator(latent_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=latent_dim),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(784, activation='tanh') \n",
    "    ])\n",
    "    return model\n",
    "    \n",
    "def build_discriminator(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(input_shape,)),  \n",
    "        Dense(128, activation='relu'), \n",
    "        Dense(1, activation='sigmoid') \n",
    "    ])\n",
    "    return model\n",
    "    \n",
    "latent_dim = 100  \n",
    "generator = build_generator(latent_dim)\n",
    "discriminator = build_discriminator(784)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feed6b53-0505-432d-8a6b-0c8627583494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Lambda\n",
    "from tensorflow.keras import Model, backend as K\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "inputs = Input(shape=(784,)) # Encoder\n",
    "h = Dense(256, activation='relu')(inputs)\n",
    "z_mean = Dense(2)(h)\n",
    "z_log_var = Dense(2)(h)\n",
    "z = Lambda(sampling, output_shape=(2,))([z_mean, z_log_var])\n",
    "decoder_h = Dense(256, activation='relu') \n",
    "decoder_mean = Dense(784, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "vae = Model(inputs, x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b9fb3a-7652-4e0c-bf88-25308b801779",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "\n",
    "sequence_length = 10  \n",
    "num_features = 5\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(sequence_length, num_features)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e207811-1815-4b3d-9c08-775255ae611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LayerNormalization, MultiHeadAttention \n",
    "\n",
    "def simplified_gpt_model(vocab_size=10000, embed_dim=256, max_length=40, num_heads=4, ff_dim=512):\n",
    "    inputs = Input(shape=(max_length,)) \n",
    "    embedding_layer = Embedding(input_dim=vocab_size, output_dim=embed_dim)(inputs)\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(embedding_layer, embedding_layer)\n",
    "    attn_output = LayerNormalization(epsilon=1e-6)(attn_output + embedding_layer)\n",
    "    ff_network = Dense(ff_dim, activation=\"relu\")(attn_output)\n",
    "    ff_network_output = Dense(embed_dim)(ff_network)\n",
    "    sequence_output = LayerNormalization(epsilon=1e-6)(ff_network_output + attn_output)\n",
    "    outputs = Dense(vocab_size, activation=\"softmax\")(sequence_output)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "    \n",
    "gpt_model = simplified_gpt_model()\n",
    "gpt_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b02370-9413-4551-840f-ccf87a9ca512",
   "metadata": {},
   "source": [
    "## Semantic Proximity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1657f-2e8e-420a-85b4-f3b8cc391b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A, B = np.array([2, 2]), np.array([3, 0])\n",
    "cos_sim = np.dot(A, B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
    "angle = np.arccos(cos_sim)\n",
    "plt.figure(figsize=(6,6))\n",
    "for vector, color, label in zip([A, B], ['red', 'green'], ['Vector A', 'Vector B']):\n",
    "    plt.quiver(0, 0, vector[0], vector[1], angles='xy', scale_units='xy', scale=1, color=color, label=label)\n",
    "plt.plot(0.5 * np.cos(np.linspace(0, angle, 100)), 0.5 * np.sin(np.linspace(0, angle, 100)), color='blue', label=f'Cosine Similarity = {cos_sim:.2f}')\n",
    "plt.axis([-1, 4, -1, 4])\n",
    "plt.axhline(0, color='black', linewidth=0.5)\n",
    "plt.axvline(0, color='black', linewidth=0.5)\n",
    "plt.grid(color='gray', linestyle='--', linewidth=0.5)\n",
    "plt.title('Cosine Similarity between Vectors')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf0af7-de0b-4351-912b-67ad23c3af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "sentence_1 = \"He turned on the light to read.\"\n",
    "sentence_2 = \"The light fabric was perfect for summer.\"\n",
    "tokens_1 = tokenizer(sentence_1, return_tensors=\"tf\")\n",
    "tokens_2 = tokenizer(sentence_2, return_tensors=\"tf\")\n",
    "outputs_1 = model(tokens_1)\n",
    "outputs_2 = model(tokens_2)\n",
    "light_index_1 = tokens_1['input_ids'][0].numpy().tolist().index(tokenizer.convert_tokens_to_ids('light'))\n",
    "light_index_2 = tokens_2['input_ids'][0].numpy().tolist().index(tokenizer.convert_tokens_to_ids('light'))\n",
    "embedding_1 = outputs_1.last_hidden_state[0, light_index_1]\n",
    "embedding_2 = outputs_2.last_hidden_state[0, light_index_2]\n",
    "cosine_similarity = tf.keras.losses.cosine_similarity(embedding_1, embedding_2, axis=0)\n",
    "cosine_similarity = -cosine_similarity.numpy()\n",
    "print(\"Cosine similarity between 'light' embeddings in the two sentences:\", cosine_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2253b648-af15-4a3d-b68c-b6c0b6a8b2e0",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b69f13-3a3f-450a-9141-5a44ed047189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(10, 128)),\n",
    "    LSTM(64),\n",
    "    Dense(10, activation='softmax') \n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "plot_model(model, show_shapes=True, show_layer_names=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296919a7-6dde-4799-aaf8-19b82e27f738",
   "metadata": {},
   "source": [
    "## Effect of temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87300395-df42-45c8-bc8c-e8e2b4e8ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.array([2, 1, 0.1, 5, 3])\n",
    "labels = ['mat' , 'tree', 'ball', 'bed', 'table']\n",
    "temperatures = [0.5, 1, 2, 4]\n",
    "\n",
    "def softmax_temperature_adjusted(logits, temperature):\n",
    "    exp_logits = np.exp(logits / temperature)\n",
    "    return exp_logits / np.sum(exp_logits)\n",
    "    \n",
    "plt.figure(figsize=(12, 8))\n",
    "for T in temperatures:\n",
    "    probabilities = softmax_temperature_adjusted(logits, T)\n",
    "    plt.plot(labels, probabilities, marker='o', label=f'Temperature = {T}')\n",
    "plt.title('Effect of Temperature on Prediction Distribution')\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Words')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4349996d-9eba-41d6-91ba-eac6773ac7a7",
   "metadata": {},
   "source": [
    "## Zero-Shot Learning Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454c2d66-a86d-4bd8-92b7-776662ed8a1e",
   "metadata": {},
   "source": [
    "### Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444efe87-32d9-4cd3-a740-e94c1fc972e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "prompt = \"Write a compelling product description for eco-friendly kitchenware emphasizing sustainability:\"\n",
    "completion = generator(prompt, max_length=100, num_return_sequences=1, temperature=0.7)\n",
    "print(\"Generated text using HuggingFace's GPT model:\", completion[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d67c03-2527-4a57-bcd0-df4815318b0c",
   "metadata": {},
   "source": [
    "### OpenAI API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1903092e-94d0-4fea-a83e-5d54ccdf6984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key = 'XXX') #insert your api key here \n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\",  \"content\": \"Write a compelling product description for eco-friendly kitchenware emphasizing sustainability:\"}],\n",
    "    max_tokens=100, n=1, temperature=0.7)\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd764c-36f9-4b9a-a85e-ee4fbc297299",
   "metadata": {},
   "source": [
    "# Creating an Effective Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9049e6f-66d8-44ca-af94-df34afcf7451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, model=\"gpt-4\", max_tokens=100, temperature=0.7, n=1):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens,\n",
    "        n=n,\n",
    "        temperature=temperature)\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac7d248-fff5-43bb-982a-446071e8ae0b",
   "metadata": {},
   "source": [
    "## Product descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d522f1-89b3-4045-826f-01cbc7a1e700",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_product_description = \"Write a captivating description for a bamboo cutlery set designed for eco-conscious consumers, emphasizing its sustainability and style.\"\n",
    "generate_response(good_product_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bff732-4897-4940-876d-61ee4c50f184",
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_product_description = \"Talk about bamboo cutlery.\"\n",
    "generate_response(poor_product_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fed23a6-12d8-4371-ae98-b2b758162ed7",
   "metadata": {},
   "source": [
    "## Blog Post Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ff6f4-125a-4d17-8bd7-9a2a26d9cf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_blog_title = \"Create an intriguing title for a blog post highlighting the top five benefits of biodegradable kitchenware for sustainable living.\"\n",
    "generate_response(good_blog_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8948b-2987-4d9a-8a43-65fc88248019",
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_blog_title = \"Write a title about kitchenware benefits.\"\n",
    "generate_response(poor_blog_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3783793c-3f79-4fcd-aa89-8b1294d85134",
   "metadata": {},
   "source": [
    "## Social Media Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b01684-64b2-4af0-8a31-5b467a4bdbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_social_media_caption = \"Create an engaging and witty Instagram caption for our latest eco-friendly kitchenware line, focusing on reducing plastic waste.\"\n",
    "generate_response(good_social_media_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3502611b-b894-4034-ad6b-316ebfb1f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_social_media_caption = \"Make an Instagram post for kitchenware.\"\n",
    "generate_response(poor_social_media_caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfcfae3-ac63-407e-9089-f4d07767d979",
   "metadata": {},
   "source": [
    "# Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab417800-031e-4367-a0bf-99d7b5b26c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slogan(prompt, temperature=1.0, top_p=1.0):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=15,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        n=3\n",
    "    )\n",
    "    return (response.choices[0].message.content, response.choices[1].message.content, response.choices[2].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ebc537-6373-4959-b3ae-4fbac499858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Create an impactful slogan for an eco-friendly clothing line that emphasizes sustainability and fashion.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73aa60d-c3ca-4524-87c3-e1d494c42e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base case\n",
    "generate_slogan(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895f5573-2063-44c4-a821-676b27bf54d5",
   "metadata": {},
   "source": [
    "## Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be502318-d210-44ff-a172-c6dc2a9dde5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High temperature\n",
    "generate_slogan(prompt, temperature=1.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28ecf1-708f-4be5-b17e-7ddac8b63238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperature\n",
    "generate_slogan(prompt, temperature=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d2725e-ca06-47c7-b2a6-b66027d412a9",
   "metadata": {},
   "source": [
    "## Top P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9e0f6-bcc6-463f-9ed6-6d9a511d2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowering Top P\n",
    "generate_slogan(prompt, top_p=0.4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
