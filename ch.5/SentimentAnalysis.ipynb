{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ff3394c-4ee4-47e1-9e6b-6f1ea760c58e",
   "metadata": {},
   "source": [
    "# Preparing Data for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18b5755-68d5-4678-9146-4bd6fd14e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('Tweets.csv') \n",
    "df.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef630438-b69f-4cba-a198-46a7fd00e579",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_colwidth\", None) \n",
    "examples_idx = df.sample(5).index # [1106, 4860, 6977, 8884, 9108] \n",
    "df_sample = df.loc[examples_idx] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b9a397-cf6e-4807-8a15-c7ae84b77518",
   "metadata": {},
   "source": [
    "## Traditional NLP pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329871d6-5cd4-49a9-be5d-ad499df309c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825db06e-8742-42b1-9ed8-e04f327c854d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import spacy \n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") \n",
    "\n",
    "def clean_text(text): \n",
    "    text = re.sub(r'@\\w+|#\\w+|https?://\\S+', '', text) \n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    return text.lower() \n",
    "\n",
    "df_sample['cleaned_text'] = df_sample['text'].apply(clean_text) \n",
    "df_sample[[\"text\", \"cleaned_text\"]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e301c8f-f191-4253-9043-1ab8374825f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_remove_stopwords(row): \n",
    "    doc = nlp(row['cleaned_text']) \n",
    "    all_tokens = [token.text for token in doc] \n",
    "    tokens_without_stop = [token.text for token in doc if not token.is_stop] \n",
    "    processed_text = ' '.join(tokens_without_stop) \n",
    "    row['all_text_tokens'] = all_tokens \n",
    "    row['without_stop_words_tokens'] = tokens_without_stop \n",
    "    row['processed_text'] = processed_text \n",
    "\n",
    "    return row \n",
    "\n",
    "df_sample = df_sample.apply(tokenize_and_remove_stopwords, axis=1) \n",
    "df_sample[['cleaned_text', 'all_text_tokens', 'without_stop_words_tokens', 'processed_text']] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbab29b-3773-4ea9-8670-20853412e3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text): \n",
    "\n",
    "    doc = nlp(text) \n",
    "    lemmatized = [token.lemma_ for token in doc] \n",
    "    return ' '.join(lemmatized) \n",
    "\n",
    "df_sample['final_text'] = df_sample['processed_text'].apply(lemmatize_text) \n",
    "df_sample[['processed_text', 'final_text']] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee31f4-2ad8-46c5-b395-a472064cd726",
   "metadata": {},
   "source": [
    "## GenAI for data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1fd269-a085-4a93-82da-f801d343b770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime \n",
    "\n",
    "sentiment_by_airline = df.groupby(['airline', 'airline_sentiment']).size().unstack().fillna(0) \n",
    "plt.figure(figsize=(14, 6)) \n",
    "sentiment_by_airline.plot(kind='bar', stacked=True, color=['red', 'yellow', 'green']) \n",
    "plt.title('Sentiment Distribution by Airline') \n",
    "plt.xlabel('Airline') \n",
    "plt.ylabel('Number of Tweets') \n",
    "plt.xticks(rotation=45) \n",
    "plt.legend(title='Sentiment') \n",
    "plt.tight_layout() \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865bac8-457a-4ec8-99ce-a6f1c1327365",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['airline_sentiment'].value_counts() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb793f95-7190-4caa-9d0e-f404da8eb860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "negative = df[df.airline_sentiment == 'negative'] \n",
    "neutral = df[df.airline_sentiment == 'neutral'] \n",
    "positive = df[df.airline_sentiment == 'positive'] \n",
    "negative_downsampled = resample(negative, n_samples=len(positive)) \n",
    "\n",
    "df_downsampled = pd.concat([negative_downsampled, neutral, positive])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fad4576-d12a-4856-b08b-ba0c5a101f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "generator = pipeline('text-generation', model='distilgpt2') \n",
    "\n",
    "def augment_text(text, augment_times=2): \n",
    "    augmented_texts = [] \n",
    "    for _ in range(augment_times): \n",
    "        generated = generator(text, max_length=60, num_return_sequences=1) \n",
    "        new_text = generated[0]['generated_text'].strip() \n",
    "        augmented_texts.append(new_text) \n",
    "\n",
    "    return augmented_texts \n",
    "\n",
    "seed_text = \"Fantastic airline service on this flight. My favorite part of the flight was\" \n",
    "augmented_examples = augment_text(seed_text) \n",
    "\n",
    "def remove_extra_spaces(text): \n",
    "    words = text.split() \n",
    "    return ' '.join(words) \n",
    "\n",
    "for example in augmented_examples: \n",
    "    print(\"------\\n\", remove_extra_spaces(example)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb331d31-7a29-4c31-b725-d86fac4c718a",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = pd.DataFrame({ \n",
    "    'text': augmented_examples, \n",
    "    'airline_sentiment': ['positive'] * len(augmented_examples) \n",
    "}) \n",
    "\n",
    "df_augmented = pd.concat([df, augmented_data], ignore_index=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df8713d-b9d4-4953-8c05-33e6db619dd0",
   "metadata": {},
   "source": [
    "# Performing sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ad73dc-8e7a-4caf-b1ed-d8056df6cd75",
   "metadata": {},
   "source": [
    "## Building your own Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81acd5-aabd-4a6e-b056-6995a1b63f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text) \n",
    "df = df.apply(tokenize_and_remove_stopwords, axis=1) \n",
    "df['final_text'] = df['processed_text'].apply(lemmatize_text) \n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000) \n",
    "\n",
    "X = tfidf_vectorizer.fit_transform(df['final_text']) \n",
    "y = df['airline_sentiment'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac009d7b-32f9-4ef3-b9a0-94d4bd7039a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "\n",
    "model = LogisticRegression(max_iter=1000) \n",
    "model.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b427d-7ead-49f8-a80f-41523b6a4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names_out() \n",
    "class_labels = model.classes_ \n",
    "\n",
    "for index, class_label in enumerate(class_labels): \n",
    "    coefficients = model.coef_[index] \n",
    "    coefficients_df = pd.DataFrame({ \n",
    "        'Feature': feature_names, \n",
    "        'Coefficient': coefficients \n",
    "    }) \n",
    "\n",
    "    coefficients_df['Absolute_Coefficient'] = coefficients_df['Coefficient'].abs() \n",
    "    coefficients_df = coefficients_df.sort_values(by='Absolute_Coefficient', ascending=False) \n",
    "    print(f\"Class: {class_label}\") \n",
    "    print(coefficients_df[['Feature', 'Coefficient']].head(10)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caee62c-1ff9-404b-90f0-d5aa3267a81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "\n",
    "y_pred = model.predict(X_test) \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee42fb31-f30b-407e-b0ce-87c3473a92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "import seaborn as sns \n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred, labels=['negative', 'neutral', 'positive']) \n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'neutral', 'positive'], yticklabels=['negative', 'neutral', 'positive']) \n",
    "\n",
    "plt.ylabel('Actual') \n",
    "plt.xlabel('Predicted') \n",
    "plt.title('Confusion Matrix') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383938a8-9de9-40d2-acac-c18a0f61a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = df[df['airline_sentiment_gold'].notnull()] \n",
    "\n",
    "X_gold = tfidf_vectorizer.transform(gold_df['final_text']) \n",
    "y_gold = gold_df['airline_sentiment_gold'] \n",
    "y_gold_pred = model.predict(X_gold) \n",
    "\n",
    "gold_df['predicted_sentiment'] = y_gold_pred \n",
    "\n",
    "misclassified = gold_df[gold_df['airline_sentiment_gold'] != gold_df['predicted_sentiment']] \n",
    "misclassified[['airline_sentiment_gold', 'predicted_sentiment', 'text', 'final_text', 'negativereason_gold']]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96032b-6e9c-4c5e-9346-75a016a652e2",
   "metadata": {},
   "source": [
    "## Using pre-trained LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda4b51-0594-498e-ada5-d2f6697d1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm \n",
    "import time \n",
    "\n",
    "filtered_df = df[df['airline_sentiment'] != 'neutral'] \n",
    "X = filtered_df['text'] \n",
    "y = filtered_df['airline_sentiment'] \n",
    "X_train_texts, X_test_texts, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\") \n",
    "start_time = time.time() \n",
    "results = [] \n",
    "\n",
    "for text in tqdm(X_test_texts, desc=\"Analyzing sentiments\"): \n",
    "    result = sentiment_pipeline(text) \n",
    "    results.append(result[0]['label'].lower())  \n",
    "\n",
    "end_time = time.time() \n",
    "total_time = end_time - start_time \n",
    "\n",
    "print(f\"Total time for analyzing {len(X_test_texts)} tweets: {total_time:.2f} seconds\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848076c-dbcf-4676-971f-6feaac9027ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, results)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72ca304-6139-4268-918d-3710e7313a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, results, labels=['negative', 'positive'])\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negative', 'positive'], yticklabels=['negative', 'positive'])\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ec3fbd-4aa0-4788-ae25-2760ff55ba1b",
   "metadata": {},
   "source": [
    "# Translating Sentiment into Actionable Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fae163-225d-4c39-8cb4-c8670c3749c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tweepy\n",
    "import tweepy\n",
    "# Replace these with your API keys and tokens\n",
    "consumer_key = 'YOUR_CONSUMER_KEY'\n",
    "consumer_secret = 'YOUR_CONSUMER_SECRET'\n",
    "access_token = 'YOUR_ACCESS_TOKEN'\n",
    "access_token_secret = 'YOUR_ACCESS_TOKEN_SECRET'\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "query = \"@YourBrandHandle -filter:retweets\"\n",
    "tweets = api.search_tweets(q=query, lang=\"en\", count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32f0f92-c8b9-4c42-a38c-929f30f7b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "    'tweet_id': tweet.id,\n",
    "    'text': tweet.text,\n",
    "    'tweet_created': tweet.created_at,\n",
    "    'tweet_location': tweet.user.location,\n",
    "    } for tweet in tweets]\n",
    "your_brand_df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7550f4af-16d1-4d8a-b102-18d59e3616c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "reviews = [\n",
    "    \"I recently purchased a sleeping bag from Optimal Hiking Gear and it exceeded my expectations.\",\n",
    "    \"The tent I bought from Optimal Hiking was damaged on arrival. Very disappointed.\",\n",
    "    \"The Optimal Hiking company makes a backpack thatâ€™s the best. I've been using mine for years without any issues.\"\n",
    "]\n",
    "for review in reviews:\n",
    "    doc = nlp(review)\n",
    "    for ent in doc.ents:\n",
    "        print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15a6f7b-540f-4b24-8ffe-2aecd40fe76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.negativereason.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72d2ff-3602-4394-8ae9-87df8b4a4b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792e7a6-9163-482a-9a50-5ff6bbe44ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud \n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2)) \n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['final_text']) \n",
    "tfidf_scores = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_matrix.sum(axis=0).tolist()[0])) \n",
    "wordcloud_tfidf = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(tfidf_scores) \n",
    "\n",
    "plt.figure(figsize=(10, 5)) \n",
    "plt.imshow(wordcloud_tfidf, interpolation='bilinear') \n",
    "plt.axis('off') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67868c96-f740-4d44-93d5-311097d3a526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  \n",
    "\n",
    "def extract_hashtags(text): \n",
    "    return re.findall(r\"#(\\w+)\", text) \n",
    "\n",
    "hashtags = sum(df['text'].apply(extract_hashtags).tolist(), []) \n",
    "hashtag_freq_dist = nltk.FreqDist(hashtags) \n",
    "wordcloud_hashtags = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(hashtag_freq_dist) \n",
    "\n",
    "plt.figure(figsize=(10, 5)) \n",
    "plt.imshow(wordcloud_hashtags, interpolation='bilinear') \n",
    "plt.axis('off') \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e471bf-0fed-4180-9b94-106362409086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "count_vect = CountVectorizer(max_df=0.95, min_df=2, stop_words='english') \n",
    "doc_term_matrix = count_vect.fit_transform(df['final_text']) \n",
    "LDA = LatentDirichletAllocation(n_components=5, random_state=42) \n",
    "LDA.fit(doc_term_matrix) \n",
    "\n",
    "for i, topic in enumerate(LDA.components_): \n",
    "    print(f\"Top words for topic #{i}:\") \n",
    "    print([count_vect.get_feature_names_out()[index] for index in topic.argsort()[-10:]]) \n",
    "    print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4fdcaf-8502-4243-a7f2-526ff8a288be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_created'] = pd.to_datetime(df['tweet_created']).dt.tz_convert(None)\n",
    "df['date'] = df['tweet_created'].dt.date\n",
    "airline_handle = \"@JetBlue\"\n",
    "airline_tweets = df[df.text.str.contains(airline_handle)]\n",
    "grouped = airline_tweets.groupby(['airline_sentiment', 'date']).agg({'tweet_id':'count', 'retweet_count':'sum'}).reset_index()\n",
    "positive_tweets = grouped[grouped['airline_sentiment'] == 'positive']\n",
    "neutral_tweets = grouped[grouped['airline_sentiment'] == 'neutral']\n",
    "negative_tweets = grouped[grouped['airline_sentiment'] == 'negative']\n",
    "plt.figure(figsize=(14, 7))\n",
    "scale_factor = 3\n",
    "for tweets, sentiment, color, linestyle in zip(\n",
    "    [positive_tweets, neutral_tweets, negative_tweets], \n",
    "    ['Positive', 'Neutral', 'Negative'], \n",
    "    ['green', 'orange', 'red'], \n",
    "    ['-', '--', '-.']\n",
    "):\n",
    "    scaled_retweet_count = tweets['retweet_count'] * scale_factor\n",
    "    plt.plot(tweets['date'], tweets['tweet_id'], linestyle=linestyle, label=sentiment, color=color)\n",
    "    plt.scatter(tweets['date'], tweets['tweet_id'], scaled_retweet_count, color=color)\n",
    "plt.title(f'Daily Sentiment Trend for {airline_handle} with Bubble Size Indicating Retweets')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Tweets')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfa0d6-884c-47af-b6e0-f308eb9503c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_of_interest = [pd.to_datetime('2015-02-22').date(), pd.to_datetime('2015-02-23').date(), pd.to_datetime('2015-02-24').date()]\n",
    "filtered_df = airline_tweets[(airline_tweets['date'].isin(dates_of_interest)) & (airline_tweets['airline_sentiment'] == 'negative')]\n",
    "top_tweets_per_date = filtered_df.groupby('date').apply(lambda x: x.nlargest(3, 'retweet_count'))\n",
    "top_tweets_per_date[['text', 'retweet_count', 'negativereason']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd76341-2cc1-46f5-b827-79566cca8681",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install folium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e97779-65d4-47dd-8640-d356a32f3eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "filtered_df = df[(df['text'].str.contains('@JetBlue') & (df['airline_sentiment'] == 'negative'))]\n",
    "filtered_df = filtered_df.dropna(subset=['tweet_coord'])\n",
    "valid_coords = []\n",
    "for coord in filtered_df['tweet_coord']:\n",
    "    try:\n",
    "        lat, long = eval(coord)\n",
    "        valid_coords.append((lat, long))\n",
    "    except (TypeError, SyntaxError, NameError):\n",
    "        continue\n",
    "if valid_coords:\n",
    "    map_center = [sum(x)/len(valid_coords) for x in zip(*valid_coords)]\n",
    "else:\n",
    "    map_center = [0, 0]\n",
    "tweet_map = folium.Map(location=map_center, zoom_start=4)\n",
    "HeatMap(valid_coords).add_to(tweet_map)\n",
    "tweet_map"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
